---
key: /2024/01/15/KafkaStudy1.html
title: Kafka - Kafka Study1
tags: eventbroker messagebroker redis kafka
--- 

# 1. `Message Broker` vs `Event Broker`

### 1) Message Broker

- `메시지 브로커`는 일반적으로 대기업들에서 대규모 메시지 기반 미들웨어 아키텍쳐에서 사용되어 왔다. `미들웨어`는 서비스하는 애플리케이션들을 보다 효율적으로 아키텍처들을 연결하는 요소들로 작동하는 소프트웨어이다.

<br>
- 예를 들어, 메시징 플랫폼, 인증 플랫폼, 데이터베이스 등이 있다.

<br>
- 또한, `메시지 브로커`는 메시지 브로커에 있는 큐에 데이터를 보내고 받는 `프로듀서`와 `컨슈머`를 통해 메시지를 통신하고 네트워크를 맺는 용도로 사용해왔다.

<br>
- `메시지 브로커`는 메시지를 받아서 적절히 처리하고나서 `즉시` 또는 `짧은 시간 내`에 `삭제되는 구조`이다.

---

<br><br>

### 2) Event Broker

- `이벤트 브로커`는 '메시지 브로커'의 구조와 다른 구조로 만들어져 있다.

<br>
- a. 이벤트 또는 메시지라고도 불리는 `레코드`인 이 장부를 딱 하나만 보관하고 `인덱스`를 통해 `개별 액세스`를 관리한다. 

<br>
- b. 업무상 필요한 시간 동안, 이벤트를 보존할 수 있다. 이것이 메시지 브로커와 가장 큰 차이점이다.

---

<br><br>

### 3) 최종 정리

- `메시지 브로커`는 데이터를 보내고, 처리하고 삭제한다. 하지만, 이벤트 브로커는 데이터를 삭제하지 않는다.

<br>
- `이벤트 브로커`가 데이터를 삭제하지 않는 이유? 
	- `이벤트 브로커`에서 '이벤트'라는 의미 때문이다. 이벤트 브로커는 서비스에서 나오는 이벤트를 마치 데이터베이스에 저장하듯이 이벤트 브로커의 큐에 저장한다.
	- 이렇게 저장함으로써 얻는 명확한 이점이 있다. 
	
	
---


<br><br>

### 4) 이벤트 브로커 이점

- 첫번째 이점은 딱 한번 일어난 이벤트 데이터를 브로커에 저장함으로서 단일 진실 공급원으로 상요할 수 있다. 

<br>
- 두번째는 장애가 발생했을 때, 장애가 일어난 지점부터 재처리할 수 있다. 

<br>
- 세번째는 많은 양의 실시간 스트림 데이터를 효과적으로 처리할 수 있다는 특징이 있다. 그외에는 MSA에서 중요한 역할을 맡을 수 있다.

<br>
- 종류** : 메시지 브로커는 보통 `Redis`, `RabbitMQ`를 이용한다. 이벤트 브로커는 `카프카`나 AWS의 `키네시스`가 대표적이다.


---

<br><br>


# 2. Kafka

### 1) Kafka 구조

- 먼저, Kafka는 Producer, Kafka, Consumer, ZooKeeper 이렇게 총 네 가지로 분리를 할 수가 있다.

<br>
- Producer는 카프카로 메시지를 보내는 역할을 한다. 그리고 그 카프카는 Producer가 보낸 메시지를 저장하는 역할을 한다.

<br>
- Consumer는 카프카에 저장되어 있는 메시지를 가져오는 역할을 한다. 그리고 그 카프카는 Producer가 보낸 메시지를 저장하는 역할을 한다.

<br>
- 그리고 이 카프카는 분산 코디네이터 시스템인 ZooKeeper와 연결을 하면서 메타데이터 관리라든지 아니면 클러스터의 노드 관리라든지 이런 것들을 ZooKeeper를 통해서 하고 있다.

---

<br><br>

### 2) 카프카 클러스터 설계 스펙 및 과정

- 그래서, 만약에 카프카 클러스터를 처음 구성한다고 하면 주키퍼 같은 경우에는 Replication 방식, 커런 방식이기 때문에 홀수를 유지시켜줘야 된다.

<br>
- 그래서, 최소 수량인 3대로 구성하는 것입니다 3대로 구성을 해줘야 된다. 그리고, 카프카 같은 경우에는 Replication 방식이 커런 방식이 아니다.

<br>
- 그래서 홀수든 짝수든 전혀 상관이 없지만 Replication-Factor를 최소 3으로 유지를 하는 경우가 많기 때문에 최소 브로커 수를, 카프카 수를 3대로 하는게 좋다.

<br>
- 그래서, 총 최소 6대의 서버가 필요하게 된다. 이 카프카 같은 경우에는 만약에 사용량이 몰려서 서버를 증설하거나 이런 경우가 발생을 하면 주키퍼 같은 경우에는 3대로 구성을 해줘야 된다.

<br>
- 그래서 총 최소 6대의 서버가 필요하게 된다. 주키퍼는 그대로 내버려 둔 채 카프카만 서버를 추가해서 이렇게 스케일아웃하는 형식으로 5대, 10대 이런 식으로 스케일아웃 할 수가 있다.

<br>
- 카카오에서는 전사 공용 카프카를 운영을 하고 있다. 처음에 오픈했을 때 당시에는 2개의 클러스터로 약 서버는 10대 정도로 시작을 했었다. 

<br>
- 그런데 지금 오늘날에 이르러서는 7개의 클러스터로 약 130여 대 정도의 서버를 운영하고 있다. 그리고 이 130여 대의 서버는 카카오에서 IDC를 여러 군데를 사용하고 있다.

<br>
- 메인으로 사용하고 있는 IDC 두 곳에 집중을 해서 각 IDC마다 주키퍼 세트를 하나씩 두고 있다. 그 주키퍼 세트와 카프카 클러스터를 연결해서 총 7개의 클러스터를 운영을 하고 있다.

<br>
- 그리고 이 서비스들은 카카오에서 많은 분들이 알고 있는 다음 거래가 될 수 있는 서비스이다.

<br>
- 이런 서비스들이 공용 카프카 클러스터 130여 대의 클러스터를 거의 모두 이용하고 있다고 보시면 된다. 그래서 이러한 카카오 서비스들이 하루에 이 카프카를 통해서 처리하고 있는 메시지 건수는
약 한 2600억 개의 메시지를 하루에 처리하고 있다.

<br>
- 이게 이 카프카를 통해서 처리하고 있는 메시지 건수는 초당 한 300만 건의 메시지를 쉬지 않고 계속 처리하고 있다고 보시면 된다. 그리고 카프카로 들어오는 데이터 사이즈를 보게 되면 하루에 240테라바이트 정도의 데이터가 카프카를 통해서 데이터가 들어오게 된다.

<br>
- 그리고 반대로 하루에 370테라바이트 정도의 사이즈가 하루에 370테라바이트 정도의 데이터가 들어오게 되고요 카프카를 통해서 바깥으로 나가게 된다. 카프카 같은 경우에는 하나의 토픽을 기준으로 프로듀서가 메시지를 보내고 여러 컨슈머들이 붙어서 가져가는 구조가 일반적인데 이렇게 인보다 아웃이 1.3배, 4배, 5배 많은 수치를 보면 카프카를 잘 활용하고 있다는 것을 알 수가 있다.

<br>
- 그리고 가동률이라는 것을 한번 계산을 해보자. 카프카를 운영한 지는 한 2년 정도 운영을 하게 되었는데 크게 클러스터 전체가 이슈가 생겼던 것은 두 건 정도가 있었다. 그래서, 그거를 약간 시간을 따져보니까 한 시간이 채 되지 않았지만 대략적인 시간으로 계산을 해봤을 때 가동률이 약 99.99% 정도 나왔다.

<br>
- 사실, 이렇게 99.99%가 나온 것은 거의 잠들지 않은 서비스이다. 그래서 이 정도만으로도 이 정도만으로도 기술이 가능한 서비스를 하고 있었다고 볼 수가 있을 것 같다.

---

<br><br>

### 3) 카프카 사용기 이슈

<br>

#### a. 첫 번째 이슈 : Shrinking ISR

<br><br>

##### a) ISR**

- `ISR`은 `In-Sync Replica`라는 첫 글자를 따서 `ISR`이라고 부르게 된다.

<br>
- ISR이 다른 어플리케이션에서 많이 쓰이는 용어는 아니다.

<br>
- 카프카에서 특화돼서 사용하는 용어 중의 하나이다. 그리고, 이게 `Replication`이 되어 있고 똑같은 구성원이 있기 때문에 이 구성원을 분류를 해줘야 된다.

---

<br><br>

##### b) Replication** : Leader, Follower

- 카프카에서는 이 `Replication` 분류를 위해서 `Leader`와 `Follower`로 분류를 하고 있다. 이렇게, 분류된 만큼 각각의 역할이 다르게 된다.

<br>
- 그래서, `Leader`는 읽고 쓰기를 주로 하고 있고 그리고 `Follower`들은 그 `Leader`랑 주기적으로 동기화하는 작업을 하게 된다.

<br>
- 그리고 ISR이 생겨난 이유 자체가 이게 가장 중요한 이유인데 ISR의 구성원만이 `Leader`의 자격을 가질 수 있기 때문이다.

<br>
- 이렇게 카프카 클러스터가 있다고 가정을 하고 거기에 브로커 3대가 있다고 가정을 하자. 

<br>
- 그리고 `Replication-Factor`는 '3'이라고 가정을 하면, 이 카프카에는 `Topic`이 3개가 있다. 

<br>
- 그리고 이 `Topic`은 `partition`이라는 단위로 쪼개질 수가 있다. 그래서 1개서부터 10개, 20개, 20개 이런 식으로 쪼개질 수가 있는데 여기서는 예제를 위해서 `partition` 하나로만 표시하면, `partition 1`은 여기 보는 것처럼 0번부터 시작하게 된다. 그래서 이거는 `Topic` 하나에 `partition` 하나를 나타내고 있는 것이다. 그리고 `Leader`와 `Follower`가 있기 때문에 `Leader`는 별도로 표시했다.

<br>
- 그리고 이 리더와 `Follower`들을 이렇게 그룹핑하고 있는 이것이 바로 ISR이라고 하는 것입니다. 그래서 브로커 1번이 예를 들어서 장애가 발생을 하게 되면 `Leader`에 문제가 생긴 것이다.

<br>
- 그렇게 되면 이런 식으로 `Follower` 중에 하나가 새로운 `Leader`로 넘어가게 된다.

<br>
- 그리고 ISR이 축소가 되었다. 이렇게 ISR이 축소되는 것을 카프카에서는 `Shrinking ISR`이라고 한다.

---

<br><br>

##### c) 실무** : 이상 감지 예시

- 실무* : 실무에서 카프카를 운영하고 있었는데 어느 날 갑자기 브로커 서버에 문제가 있냐 아니면 이런 식의 문의가 왔었다. 이벤트로 메시지를 보내는데 메시지가 잘 안 간다 아니면 메시지를 가져오는데 잘 못 받는다 등의 이런 식으로 문의가 왔었다.

<br>
- 그래서 한 번 서버 전체적으로 상태를 봤는데 알람은 전혀 안 왔기 때문에 서버 전체적으로 봤는데 서버 다운된 것도 하나도 없이 모두 잘 동작을 하고 있었다.

<br>
- 그런데 보통 그래프, 그래프 하나를 이용해서 이렇게 그래프를 보는데 '인'이랑 '아웃' 쪽에 데이터가 그래프가 약간 좀 이상한 현상이 보이기 시작했다.

<br>
- 그래서 뭔가 이상을 감지하고 브로커 서버에 전체 로그인을 해서 로그를 보기 시작했지만 여기서 보시면 여기 로그 레벨이 INFO라고 찍혀 있는 부분을 보면 다음에도 주의해야 한다.

<br>
- 카프카에서는 로그 레벨이 총 3단계로 나눠져 있다. `INFO`, `WARNING`, `ERROR` 이렇게 나눠져 있는데 `INFO`는 정말 말 그대로 `INFO` 레벨이다. `INFO`성 로그다. `WARNING`은 경고성 그리고 `ERROR`는 정말 카프카에 문제가 생겼을 때 발생한 로그다.

<br>
- 처음에는 이 `INFO`성 로그는 제외하고 `WARNING` 또는 `ERROR` 메세지가 브로커 로그에 한 줄 기록되자마자 바로 알림을 받을 수 있도록 알람 설정을 해 놨었는데 지금 보는 것처럼 INFO성 로그였기 때문에 알람을 미리 받지 못했다.

<br>
- 그리고 이 문제성 로그를 보면, 여기 브로커 5번에서 `Shrinking ISR`이 발생을 했고 이 토픽에 대해서 이렇게 축소되는 이런 이슈가 발생을 했던 것이다. 그래서, 이거를 약간 시각화해서 보면은 클러스터 내에 브로커들이 여러 대가 있었는데 브로커 5번이 오동작으로 인해서 모든 `ISR` 자기가 가지고 있던 파티션에 대해서 모든 `ISR`을 축소하는 현상이 발생했다.

<br>
- 그리고 이 브로커 5번이 모든 `ISR`을 축소하는 현상이 발생했다. 이 브로커 5번이 자기가 가지고 있던 파티션에 대해서 리더를 다 가져가 버렸기 때문에 다른 브로커들은 이 브로커 5번과 리플리케이션을 해야 되는데 이 리플리케이션이 안 되는 이런 에러가 발생을 하고 있었 것이다.

<br>
- 그래서 이 증상은 그 당시에 사용하고 있었던 0.10.1.0의 버그였다. 그리고 이 내용을 확인했을 때 이미 상위 버전이 릴리즈되어서 이 버그에 대해서는 픽스가 되었던 상황이었다.

<br>
- 그래서, 이 버그에 대해서는 픽스가 되었던 상황이었다.

<br>
- 여기서 한 가지 말하고 싶은 것은 브로커에서 로그가 `WARNING` 또는 `ERROR` 로그만 이렇게 감지를 한다고 하더라도 이렇게 `INFO` 로그가 왜 발생했는지에 대해서 조금 더 한번 고민을 해 보시면 나중에 이슈가 생기더라도 트러블 슈팅하는데 도움될 것입니다.

---

<br><br>

##### d) 카프카 버전 업그레이드 방식**

- 그래서, 결국에 이 버전을 버리고 상위 버전으로 업그레이드를 해야 되는데 이 카프카에서 버전 업그레이드를 하는 방식은 두 가지가 있다.

<br><br>

##### A. 업그레이드 방식 : 다운타임 유무**

- 첫 번째로는 다운타임을 가질 수 있는 환경이랑 그리고 다운타임을 가질 수 없는 환경 이렇게 크게 두 가지로 나눌 수가 있는데 다운타임을 가질 수 있는 환경은 정말 심플하게 작업을 할 수가 있다.

<br>
- 모든 브로커를 다 내려버리고 최신 버전으로 업그레이드 한 다음에 브로커를 그냥 실행해 주면 업그레이드가 끝나게 된다.

<br>
- 하지만, 아마 일반적인 보통의 상황들이 다운타임을 가질 수 없는 환경일 것이다. 이런 상황에서는 클러스터 내 브로커를 한 대씩 내렸다가 버전 업그레이드하고 올리고 내렸다가 버전 업그레이드하고 올리고 이런 식으로 작업을 진행해야 한다.

<br>
- 그래서, 전체 클러스터는 다운되지 않은 상태에서 한 대씩 한 대씩 버전 업그레이드를 진행해야 된다.

<br>
- 그래서, 카카오에서는 크리티컬한 이슈가 방금 아까 그런 버그같은 이런 일들이 발생을 하거나 아니면 카프카의 새로운 신규 기능이 추가되어서 그 기능을 써야만 하는 경우, 그럴 때를 다운타임 없이 버전 업그레이드를 진행을 하고 있다.


---

<br><br>

#### b. 두번째 이슈 : 전원 이슈

- 그리고 두 번째 이슈는 전원 이슈였다. 일반적으로 서버들을 IDC에 두게 된다.

<br>
- 그리고 그 IDC에는 랙이라고 불리우는 공간에다가 서버를 두게 되는데 랙은 뭐 이런 선반 같은 개념이다.

<br>
- 좀 더 자세히 보면 이런 식으로 이렇게 랙 1, 2, 3이 있다. 그리고, 이 랙 1, 2, 3에다가 이제 원하는 서버들을 자유롭게 꽂게 된다.

<br>
- 브로커 서버, DB 서버, 웹 서버 뭐 이런 식으로 랙에다가 서버를 꽂게 됩니다. 근데 카프카 같은 경우에는 이제 클러스터 개념이기 때문에 3대가 있다고 가정을 해보고 그 브로커 1, 2, 3을 랙 1번에다가만 몰아놨다고 가정하자.

<br>
- 사실 이렇게 구성을 하게 되면은 랙 1번이 문제가 생기게 되면 클러스터 전체가 다운되는 문제가 생기게 된다.

<br>
- 그렇기 때문에 각 랙마다 브로커 서버를 분산해서 한 대씩 한 대씩 배치하는 것이 효율적인 배치이다.

<br>
- 근데 그 랙 전원장애가 발생을 했다고 앞서 말했었는데 IDC의 랙 전원장애가 발생했을 때 랙 한 열이, 그러니까 약 한 15개 정도의 렉의 전원이 한 번에 다다다닥 전원이 나간 상황이다.

<br>
- 그러니까, 랙 15개 내에 클러스터가 한 10개 이렇게 구성이 되어 있었는데 거기에 랙 전원이 전부 다 나가면서 모든 브로커가 다운되는 상황을 겪게 된 것이다.

<br>
- 그래서 이렇게 모든 브로커가 다운되는 경우가 흔하진 않지만 이런 경우에 대해서도 한 번쯤은 고민을 해야 한다.


---

<br>

##### a) 다운타임이 없는 경우의 전원 이슈 해결 예시

<br>
- 예를 들어, 이렇게 랙 1, 2, 3이 있고 거기에 브로커 1, 2, 3이 있다고 가정을 해보자.

<br>
- 그리고 거기에 'peter-topic'이라는 게 있고 메시지 'a'를 가지고 있다고 가정을 해보자. 그리고 노란색은 리더고 팔로워 두 개가 있다고 가정을 하자.

<br>
- 그런데 여기서 랙 1번에 전원장애가 발생을 한 것이다. 이렇게 전원장애가 발생을 하게 되면 팔로워에게 새로운 리더가 넘어가게 되고 다음 메시지인 'b'라는 메시지를 받게 된다.

<br>
- 그리고 렉 2번도 전원장애가 발생을 했고 다음 리더인 마지막 브로커로 리더가 넘어가게 된다. 그리고 마지막 브로커는 다음 메시지인 'c'라는 메시지를 받게 된다.

<br>
- 그런데 하나 남은 브로커도 역시 전원장애를 피해갈 수는 없다.

<br>
- 렉 3번에 전원장애가 발생을 하게 된다. 그럼 전원장애가 발생한 이후에 렉 3번이 가장 먼저 복구가 되게 되면 마지막 리더가 그냥 바로 새로운 리더가 되는 것이다.

<br>
- 계속 이어지는 리더가 되는 것이다. 그러고 나서 다음 메시지인 'd'라는 메시지를 받게 되고 나머지 장애에 빠져 있었던 브로커 2번과 1번이 복구가 됐을 때, 렉 3번이 전원장애가 발생을 하게 된다.

<br>
- 이미 리더가 있기 때문에 그 리더로부터 모든 메시지를 동기화하게 된다. 결국에는 전원장애로 인해서 모든 브로커가 다운됐음에도 불구하고 메시지 손실은 하나도 없게 되는 현상이 있다. 하지만 가장 먼저 다운됐었던 브로커 1번이 가장 먼저 복구가 됐다고 가정을 해보자.

<br>
- 그리고 이 브로커 1번이 바로 리더가 아무도 없기 때문에 즉시 리더의 역할을 하게 된다. 그러고 나서 메시지 d를 받게 되는 거죠. 다음에 랙 2번과 3번의 문제가 해결돼서 브로커가 올라왔을 때 이미 브로커 1번이 리더 역할을 하고 있기 때문에 모든 메시지를 리플리케이션 하게 된다. 결국 메시지 'b'와 'c'는 손실되게 된다.


---

<br>

##### b) 결론


<br>
- 사실 여기에서는 서비스의 영속성을 우선시 할 건지 아니면 데이터의 정합성을 우선시 할 건지 선택의 문제지 정답이 있는 것은 아니다.

<br>
- 근데 마지막 리더를 기다리는 옵션으로 설정을 해서 사용할 경우에 이렇게 모든 클러스터가 다운됐음에도 불구하고 메시지 손실을 없게 할 수가 있다.

<br>
- 하지만 우리는 최악의 시나리오도 고려를 해야 됩니다. 만약에 마지막 리더였던 서버가 전원장애 이후에 올라오지 않는 상황도 고려를 해야 되는 거죠.

<br>
- 뭔가 메인보드가 문제가 있다든지 아니면 뭔가 파워 서플라이가 문제가 있다든지 그래서 만약에 올라오지 않게 되면 다른 팔로우들은 이미 올라와 있는데도 마지막 리더가 올라오지 않기 때문에 장애 시간이 계속해서 길어질 수가 있다.

<br>
- 카카오에서는 전사 공용으로 사용하고 있기 때문에 장애가 발생을 했을 때, 최대한 장애 타임을 줄이고 빠르게 서비스에 투입되는 것이 최우선이었다.

<br>
- 그래서 이런 랙 전원장애가 발생을 했을 때, 서비스의 영속성을 우선시 했기 때문에 일부 메시지는 손실이 되었고 대신에 빠른 서비스 투입이 가능했었다.

<br>
- 그래서, 이런 사항을 잘 고려하셔서 용도에 맞게 카프카를 운영하시면 사용하는데 도움이 될 것 같다.

---

<br><br>

### 4) Producer

#### a. Producer 개념

- 먼저, `Producer`에 대해서 알아보면 다음과 같다. `Producer`는 `partition`의 리더로 메시지를 전달하는 역할을 하게 되고요. 그리고 메시지를 전달할 때 특정 `partition` 또는 랜덤 `partition`으로 전송을 하게 된다.

<br>
- 그리고 빠른 전송 속도가 보장이 되어야 효율성이 좋은 배치 처리가 가능하다. 그리고 설정을 통해서 배치 크기나 지연 시간 이런 것들이 조정이 가능하다.

<br>
- 배치나 지연 시간을 조정하는 방법을 다음과 같다.

---

<br>

##### a) ACK

- 이 `ACK`도 `Producer`가 할 수 있는 설정 중의 하나이다. 근데 `ACK` 같은 경우에는 메시지 손실과 직접적인 관련이 있는 옵션 중의 하나입니다. 그래서, 이 `Producer`에서 `ACK 옵션`이 가장 중요하다.

<br>
- `ACK`의 값은 `0`, `1`, `All` 총 세 가지가 있다.

<br>
- `ACK 0`은 매우 빠르게 전송할 수 있지만 `partition`의 `Leader`로는 많은 상황이 있다. `partition`의 `Leader`가 받았는지 확인하지 않는다. 즉, 내가 메시지를 보내고 난 다음에 상대방이 받았는지 확인을 하지 않고 바로 보내고 나의 역할은 끝난 것이다. 바로 다음 메시지를 보내게 된다. 그래서, 이런 경우에는 메시지 손실 위험이 되게 높다. 그런데, 이게 비율로 따지게 되면 사실 브로커도 아까 보신 것처럼 99.99% 거의 다운타임에서 다운되는 현상 없이 운영되고 있다. `Producer`도 거의 다운되는 현상 없이 계속 메시지를 쏘기 때문에 퍼센테이지로 따지면 그렇게 큰 수치가 나는 것은 아니다.

<br>
- 다음은 `ACK 1`입니다. 메시지 전송도 빠른 편이고 그리고 `partition`의 `Leader`가 받았는지 메시지 내가 보낸 메시지를 받았는지 확인을 한다. 가장 많이 사용되는 옵션 중에 하나다. 그리고 최근에 사용되는 `Log Stash`, `File Beat` 이런 어플리케이션(`ELK`에서 많이 사용됨)의 대부분의 기본값이 이 `ACK 1`이다. 

<br>
- 그리고, 마지막으로 `ACK ALL`은 메시지 전송은 가장 느리지만 이 메시지 전송이 가장 느린 이유는 팔로워도 메시지를 받았는지 확인을 하기 때문이다. 하지만, 그렇게 느린 상태지만 손실 없는 메시지 전송이 가능하다.

<br>
- 그런데, 여기서 한 가지 좀 약간 의아한 궁금증이 있을 수 있다. `ACK 1`이 분명히 `Leader`가 받았는지 확인을 했는데 왜 `ACK ALL`이라는 옵션이 있어서 손실 없는 메시지 전송이 가능한지 이거에 대해서 좀 자세히 보면 다음과 같다.

<br>
- 예를 들어, `Producer`가 `ACK 1`이라는 값으로 메시지를 보낸다고 가정을 해보자. 그리고 이렇게 브로커가 3대가 있고 거기에는 `Peter-Topic`이라고 똑같은 `Topic`이 3개가 있다. 그리고, 마찬가지로 `Leader`와 `Follower`가 있게 된다. 그리고 `Producer`는 `Leader`에게 메시지를 받았는지 확인을 하게 된다.

<br>
- 그리고 `Leader`는 메시지 A라는 메시지를 전달하게 된다. `Leader`는 메시지를 저장하게 된다. 그리고 저장한 상태에서 내가 메시지를 잘 받았다고 `Producer`에게 알려준다. `ACK`를 보내주게 되죠. 그리고 이 `Producer` 동작과 달리 `브로커 내부 동작`에 의해서 `Follower`들과 `Replication`이 일어나게 된다.

<br>
- 그리고 `Producer`는 다음 메시지인 B라는 메시지를 전송하게 된다. 그리고 `Leader`는 그 메시지를 전달하게 된다. 그리고 `Leader`는 그 메시지를 받고 난 다음에 `ACK`를 날려주게 된다. 그리고 나서 내부적으로 `Replication`이 일어나려고 하려는 찰나 그러니까 `Producer`한테 `ACK`는 보내줬고 내부적으로 `Replication`이 일어나려는 찰나 사실 이거는 0.00밀리세컨드 단위에 일어날 수 있는 일이긴 하다. 이 찰나에 브로커 1번이 다운되어 버리는 것이다. 다운 현상이 발생하게 되면 리더가 없어졌기 때문에 `Follower` 중에 하나가 새로운 `Leader` 역할을 하게 된다. 

<br>
- 그리고 `Producer`는 `Leader`에게 다음 메시지인 C라는 메시지를 보낸다. 내가 B라는 메시지까지 보내고 `ACK`를 받았기 때문에 C라는 메시지를 전송하게 되는 거죠. 프로듀서는 C라는 메시지를 저장하게 되고 `ACK`를 보내게 된다. 그리고 D라는 메시지 받게 되고 그리고 나서 브로커 1번이 원래 `Leader`였던 브로커 1번이 장애가 복구돼서 올라왔을 때, 자기가 올라오고 나니까 이미 `Leader`가 있다. 그러니까 그 `Leader`로부터 `Replication`이 일어나게 된다.

<br>
- 결과적으로 `Producer`가 `Leader`가 받았는지 매번 체크를 하고 `ACK`를 받았지만 결과적으로 이렇게 메시지 손실이 발생을 하게 된다. 사실 이거 비율로 따지면 99.몇 퍼센트 이렇게 나올 것 같긴 한데 만약에 정말 중요한 이런 0.몇 퍼센트의 오차도 허용하고 싶지 않다고 한다면, `Producer`를 사용할 때, `ACK`를 `All`이라는 값을 사용해서 사용하시게 되면 손실 없는 메시지 전송이 가능하다.

---

<br><br>

#### b. 운영 단계 : Key 옵션 이슈

- 다음으로 운영하다 보니까 특정 부서에서 특정 `partition`이 이상하다는 메시지가 왔었다.

<br>
- `브로커에 문제가 있습니까?` 이런 식으로 얘기가 왔는데 상태를 보니까 모든 상태는 이상한 것들이 전혀 없었다. 브로커도 이상이 없었고 다른 `Topic`들도 이상이 전혀 없었다.

<br>
- 그런데 그 `Topic`이 이상하다는 그 `Topic`만 그래프를 보니까  `partition`별로 균등하게 분배가 안 되는 것을 확인을 할 수가 있었다. 지금 이렇게 특정 일정 시간 동안 한쪽 `partition`으로 이렇게 메시지들이 몰리는 상황을 볼 수가 있다. 그래서, 확인하다 보니까 해당 부서에서는 `Producer`에 `Key`라는 옵션을 넣어서 사용하고 있었던 것이었다.

<br>
- `Key`라는 옵션이 뭔지 간단하게 알아보면 다음과 같다. 이 `Producer`가 그 `Topic`의 `partition`으로 메시지를 보내게 될 때, `Key`라는 옵션을 사용할 수가 있다.

<br>
- 예를 들어서, `Key`에 'a'라는 옵션을 사용할 수 있게 되고 'a'라는 옵션을 줬는데 이 'a'라는 옵션을 줘서 메시지를 쏘게 되면 이 `partition`이 두 개가 있음에도 불구하고 'a'에 해당하는 하나의 `partition`으로만 메시지를 보내게 된다.

<br>
- 그래서 그 메시지 내용 자체는 중요하지 않다. 대신, 그 `Key`값이 'a'라고 시작되는 `Key`값만 정해주게 되면 'a'라는 `Key`값은 `특정 partition`의 하나의 `partition`으로만 메시지를 전송하게 된다.

<br>
- 그런데 이 `Key`값 자체가 그 필수 값이 아니라 만약에 `Key`값을 주지 않게 되면 `Key`값에 `none`이라고 표시가 된다. 그래서 `Key`값을 빼버리면 그 `Topic`의 `partition`으로 랜덤하게 `Round Robin` 형태로 균등하게 분배해서 보내게 된다. 그래서 여기처럼 `Key`값이 `none`이라고 나오게 되고, 메시지를 균등하게 보내게 된다. 그래서 아까 같은 이슈 때, 그 `Key`값을 굳이 필요하지 않았기 때문에 `Key`값을 제거하고 나서는 이 뒤에 빨간 테두리에 보시는 것처럼 그 `Topic partition`별로 이렇게 균등하게 분산이 되는 것을 알 수가 있다.

<br>
- 그래서 `Producer` 옵션을 사용하실 때, 그 `Key`값을 필요로 하시면 `특정 partition`으로만 보내야 된다라는 뭐 그런 게 필요하시면 `Key`값을 넣어서 사용면 되고 그렇지 않으면 대부분 그냥 빼버리고 사용하면 이렇게 균등하게 분산시켜서 메시지를 보낼 수가 있다.

---

<br><br>

### 5) Consumer

- `Consumer`는 그 `partition`의 `Leader`에게 `Fetch` 요청하는 역할을 한다.

---

<br>

#### a) 오프셋

- 그리고 컨슈머는 위치를 기록하고 있는 그 오프셋 그 오프셋으로부터 메시지를 가져오는 역할을 하게 된다. 이 오프셋은 그러니까 쉽게 얘기하면 그 포지션 위치 정보라고 보시면 된다. 그 숫자 정보라고 보시면 된다. 카프카로 모두 표시되어 있다. 그래서 컨슈머는 메시지 내용을 보고 가져오는 게 아니라 저기 그 오프셋이라고 불리는 자리 번호를 보고 메시지를 가져오게 된다.

- 그리고 컨슈머의 목적은 컨슈머가 가능한 최대 속도로 가져올 수 있도록 하는 것아다. 즉, 프로듀서는 그 프로듀서가 카프카로 메시지를 보낼 때는 푸시 방식이다. 그래서 프로듀서가 자기가 쏘는 속도에 따라서 카프카가 받아주는 방식이다.

- 그리고 카프카는 카프카에서 컨슈머 구간은 사실 카프카가 이걸 푸시 방식으로 할지 풀 방식으로 할지 고민을 했는데 카프카가 푸시 방식으로 하게 되면 컨슈머가 메시지를 가져가는 거를 카프카가 속도를 제어하게 된다. 그래서 이거는 약간 불합리하다고 생각을 해서 카프카에서 메시지를 가져가는 컨슈머는 풀 방식으로 가져가게 된다. 그래서 컨슈머 리소스가 최대한 자기가 허용하는 범위 내에서 최대한 빠른 속도로 카프카로부터 메시지를 가져갈 수 있는 구조이다.

- 그래서 메시지를 가져가는 오프셋과 관련된 내용을 잠시 살펴보면 그 토픽의 파티션에 하나의 파티션이라고 가정을 해보고 그 안에 메시지가 ABCDE가 들어있다. 그리고 이 ABCDE는 오프셋 0, 1, 2, 3, 4 이런 식으로 위치가 기록이 되어 있다.

- 그리고 이 컨슈머는 메시지 ABCDE 메시지를 보고 가져오는 게 아니라 오프셋 0번, 오프셋 1번, 오프셋 2번, 오프셋 3번 이 순서대로 메시지를 가져오게 된다. 그래서 파티션이 하나인 경우에는 컨슈머가 메시지를 가져올 때 정확하게 순서가 보장된다.

- 근데, 카프카 같은 경우에는 하나의 토픽을 여러 개의 파티션으로 나눠서 사용하는 게 일반적인데 두 개 또는 세 개 뭐 이런 식으로 나눠서 사용을 하게 된다.

- 일반적으로 처음에 좀 약간 혼돈스러웠던 개념인데 컨슈머가 파티션 0번에 오프셋 0번 메시지 가져오고 파티션 1번에 0번 오프셋 메시지 가져오고 이런 식으로 순서대로 하나하나 가져오기를 원했는데 컨슈머는 컨슈머는 파티션의 번호는 전혀 관여하지 않고 단지 그 파티션의 오프셋 순서만 보장하게 돼 있다.

- 그 B라는 B와 C라는 메시지를 가져왔는데 그 파티션 1번에 대해서 오프셋 순서대로 가져왔던 경우에 A랑 D라는 메시지를 가져왔는데 파티션 0번에 대해서 오프셋 순서대로 가져왔다. 그 파티션에 대해서는 오프셋 순서대로 가져오지만 파티션 순서는 전혀 관여를 하지 않는다.

- 그래서 이렇게 순서가 파티션을 여러 개 사용하게 되면 메시지 순서가 약간 꼬일 수가 있게 되는데 이러한 경우는 만약에 이러한 게 싫다면 그 메시지에다가 특정 필드에 타임 스탬프라던지 이런 것들을 넣어서 저장하면서 저장할 때, 순서를 정렬해서 저장을 하게 되면 메시지 순서대로 저장할 수가 있게 된다.

---